{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary\n",
    "=======\n",
    "Defines a penalized ML estimator for Gaussian Mixture Models, using EM\n",
    "\n",
    "EM = Expectation-Maximization.\n",
    "\n",
    "Provides a CONCRETE implementation of an sk-learn-like estimator API\n",
    "\n",
    "* fit\n",
    "* score\n",
    "* get_params\n",
    "* set_params\n",
    "\n",
    "Examples\n",
    "========\n",
    ">>> np.set_printoptions(suppress=False, precision=3, linewidth=80)\n",
    ">>> D = 2\n",
    "\n",
    "## Verify that variance penalty works as expected\n",
    "# Empty components (with no assigned data) should have variance equal to the intended \"mode\" of the penalty\n",
    "# We'll use a mode of 2.0 (so stddev = sqrt(2.0) = 1.414...)\n",
    ">>> gmm_em = GMM_PenalizedMLEstimator_EM(K=3, D=2, seed=42, variance_penalty_mode=2.0)\n",
    ">>> empty_ND = np.zeros((0,D))\n",
    ">>> log_pi_K, mu_KD, stddev_KD = gmm_em.generate_initial_parameters(empty_ND)\n",
    ">>> calc_neg_log_lik(empty_ND, log_pi_K, mu_KD, stddev_KD)\n",
    "-0.0\n",
    ">>> gmm_em.fit(empty_ND, verbose=False)\n",
    ">>> gmm_em.stddev_KD\n",
    "array([[1.414, 1.414],\n",
    "       [1.414, 1.414],\n",
    "       [1.414, 1.414]])\n",
    "\n",
    ">>> N = 25; K = 3\n",
    ">>> prng = np.random.RandomState(8675309)\n",
    ">>> x1_ND = 0.1 * prng.randn(N, D) + np.asarray([[0, 0]])\n",
    ">>> x2_ND = 0.1 * prng.randn(N, D) + np.asarray([[-1, 0]])\n",
    ">>> x3_ND = np.asarray([[0.2, 0.05]]) * prng.randn(N, D) + np.asarray([[0, +1]])\n",
    ">>> x_ND = np.vstack([x1_ND, x2_ND, x3_ND])\n",
    ">>> gmm_em = GMM_PenalizedMLEstimator_EM(\n",
    "...     K=3, D=2, seed=42, variance_penalty_mode=2.0, max_iter=1)\n",
    "\n",
    ">>> gmm_em.stddev_KD = 0.1 * np.ones((K,D))\n",
    ">>> gmm_em.stddev_KD[-1] = [0.2, 0.05]\n",
    ">>> gmm_em.mu_KD = np.asarray([[0, 0], [-1., 0], [0, 1.]])\n",
    ">>> gmm_em.log_pi_K = np.log(1./3 * np.ones(K))\n",
    ">>> gmm_em.estep__calc_r_NK(x_ND[:3])\n",
    "array([[1.000e+00, 5.336e-25, 3.829e-75],\n",
    "       [1.000e+00, 2.151e-17, 3.063e-97],\n",
    "       [1.000e+00, 4.367e-19, 1.984e-90]])\n",
    ">>> gmm_em.estep__calc_r_NK(x_ND[-3:])\n",
    "array([[4.752e-25, 1.362e-38, 1.000e+00],\n",
    "       [2.278e-17, 7.579e-46, 1.000e+00],\n",
    "       [4.189e-22, 4.117e-34, 1.000e+00]])\n",
    ">>> gmm_em.fit(x_ND, verbose=False)\n",
    ">>> np.exp(gmm_em.log_pi_K)\n",
    "array([0.333, 0.333, 0.333])\n",
    ">>> gmm_em.mu_KD\n",
    "array([[-0.007,  0.01 ],\n",
    "       [-1.008,  0.009],\n",
    "       [-0.005,  1.005]])\n",
    ">>> gmm_em.stddev_KD\n",
    "array([[0.076, 0.091],\n",
    "       [0.098, 0.103],\n",
    "       [0.24 , 0.042]])\n",
    "\n",
    ">>> gmm_em = GMM_PenalizedMLEstimator_EM(\n",
    "...     K=3, D=2, seed=42, variance_penalty_mode=2.0, max_iter=1000,\n",
    "...     do_double_check_correctness=True)\n",
    ">>> gmm_em.fit(x_ND, verbose=False)\n",
    ">>> np.exp(gmm_em.log_pi_K)\n",
    "array([0.333, 0.333, 0.333])\n",
    ">>> gmm_em.mu_KD\n",
    "array([[-1.008,  0.009],\n",
    "       [-0.005,  1.005],\n",
    "       [-0.007,  0.01 ]])\n",
    ">>> gmm_em.stddev_KD\n",
    "array([[0.098, 0.103],\n",
    "       [0.24 , 0.042],\n",
    "       [0.076, 0.091]])\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "from scipy.special import logsumexp\n",
    "import scipy.optimize\n",
    "import time\n",
    "\n",
    "from GMM_PenalizedMLEstimator import GMM_PenalizedMLEstimator\n",
    "from GMM_PenalizedMLEstimator import calc_neg_log_lik\n",
    "\n",
    "class GMM_PenalizedMLEstimator_EM(GMM_PenalizedMLEstimator):\n",
    "    \"\"\" Maximum Likelihood Estimator for Gaussian Mixtures, trained with EM.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of components\n",
    "    D : int\n",
    "        Number of data dimensions\n",
    "    seed : int\n",
    "        Seed for random number generator used for initialization\n",
    "    variance_penalty_mode : float\n",
    "        Must be positive.\n",
    "        Defines mode of penalty on variance.\n",
    "        See calc_penalty_stddev module.\n",
    "    variance_penalty_spread : float,\n",
    "        Must be positive.\n",
    "        Defines spread of penalty on variance.\n",
    "        See calc_penalty_stddev module.\n",
    "    max_iter : int\n",
    "        Maximum allowed number of iterations for training algorithm\n",
    "    ftol : float\n",
    "        Threshold that determines if training algorithm has converged\n",
    "        Same definition as `ftol` setting used by scipy.optimize.minimize\n",
    "\n",
    "    Additional Attributes (after calling fit)\n",
    "    -----------------------------------------\n",
    "    log_pi_K : 1D array, shape (K,)\n",
    "        GMM parameter: Log of mixture weights\n",
    "        Must satisfy logsumexp(log_pi_K) == 0.0 (which means sum(exp(log_pi_K)) == 1.0)\n",
    "    mu_KD : 2D array, shape (K, D)\n",
    "        GMM parameter: Means of all components\n",
    "        The k-th row is the mean vector for the k-th component\n",
    "    stddev_KD : 2D array, shape (K, D)\n",
    "        GMM parameter: Standard Deviations of all components\n",
    "        The k-th row is the stddev vector for the k-th component\n",
    "    history : dict of lists\n",
    "        Access performance metrics computed throughout iterative training.\n",
    "        history['iter'] contains integer iteration count at each checkpoint\n",
    "        history['train_loss'] contains training loss value at each checkpoint\n",
    "        history['valid_score_per_pixel'] contains validation score at each checkpoint\n",
    "            Normalized \"per pixel\" means divided by total number of observed feature dimensions (pixels)\n",
    "            So that values for different size datasets can be fairly compared.\n",
    "\n",
    "    Inherits\n",
    "    --------\n",
    "    * Constructor __init__() from GMM_PenalizedMLEstimator_LBFGS parent class\n",
    "    * Initialization method generate_initial_parameters() from parent as well\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def calc_EM_loss(self, r_NK, x_ND):\n",
    "        ''' Compute the overall loss function minimized by the EM algorithm\n",
    "\n",
    "        Includes three additive terms:\n",
    "        * Negative of the expected complete likelihoods E_q[ log p(x,z)]\n",
    "        * Negative of the entropy of the assignment distribution q(z|r)\n",
    "        * Penalty on the standard deviation parameters\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        r_NK : 2D array, shape (N, K)\n",
    "            Parameters that define the approximate assignment distribution q(z)\n",
    "            The n-th row r_NK[n] defines the K-length vector r_n that is non-negative & sums to one.\n",
    "            Can interpret r_NK[n,k] as the probability of assigning cluster k to n-th example\n",
    "            Formally, the n-th example's assignment distribution is given by:\n",
    "                q(z_n | r_n) = CategoricalPMF(z_n | r_n[0], r_n[1], ... r_n[K-1])\n",
    "        x_ND : 2D array, shape (N, D)\n",
    "            Dataset of observed feature vectors\n",
    "            The n-th row x_ND[n] defines a length-D feature vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss_em : float\n",
    "            scalar value of the loss of provided x and r arrays\n",
    "            Uses this object's internal GMM params (self.log_pi_K, self.mu_KD, self.stddev_KD)\n",
    "        '''\n",
    "        log_prior = np.sum(np.dot(r_NK, self.log_pi_K))\n",
    "        log_lik = 0.0\n",
    "        for k in range(self.K):\n",
    "            log_lik_k_N = np.sum(stats.norm.logpdf(x_ND, self.mu_KD[k], self.stddev_KD[k]), axis=1)\n",
    "            log_lik += np.inner(r_NK[:,k], log_lik_k_N)\n",
    "        entropy = -1.0 * np.sum(r_NK * np.log(r_NK + 1e-100))\n",
    "        penalty_stddev = self.calc_penalty_stddev()\n",
    "        return -1.0 * (log_prior + log_lik + entropy) + penalty_stddev\n",
    "\n",
    "    def estep__calc_r_NK(self, x_ND):\n",
    "        ''' Perform E-step to update assignment variables r controling q(z | r)\n",
    "\n",
    "        Returned value will optimize the EM loss function for r given fixed current GMM parameters\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_ND : 2D array, shape (N, D)\n",
    "            Dataset of observed feature vectors\n",
    "            The n-th row x_ND[n] defines a length-D feature vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        r_NK : 2D array, shape (N, K)\n",
    "            The n-th row r_NK[n] defines the K-length vector r_n that is non-negative & sums to one.\n",
    "            Can interpret r_NK[n,k] as the probability of assigning cluster k to n-th example\n",
    "            Formally, the n-th example's assignment distribution is given by:\n",
    "                q(z_n | r_n) = CategoricalPMF(z_n | r_n[0], r_n[1], ... r_n[K-1])\n",
    "        '''\n",
    "        N = x_ND.shape[0]\n",
    "        r_NK = np.zeros((N, self.K))\n",
    "        # r_NK = np.exp((self.log_pi_K[np.newaxis, :]+np.sum(stats.norm.logpdf(x_ND[:, np.newaxis, :], self.mu_KD, self.stddev_KD), axis=2))-\n",
    "                    # logsumexp(self.log_pi_K[np.newaxis, :]+np.sum(stats.norm.logpdf(x_ND[:, np.newaxis, :], self.mu_KD, self.stddev_KD), axis=2), axis=1, keepdims=True))\n",
    "        # log_r_NK = self.log_pi_K[np.newaxis, :]+np.sum(stats.norm.logpdf(x_ND[:, np.newaxis, :], self.mu_KD, self.stddev_KD), axis=2)\n",
    "        # log_r_NK = log_r_NK + 1e-16\n",
    "        # log_r_NK = log_r_NK - logsumexp(self.log_pi_K[np.newaxis, :]+np.sum(stats.norm.logpdf(x_ND[:, np.newaxis, :], self.mu_KD, self.stddev_KD), axis=2), axis=1, keepdims=True)\n",
    "        # r_NK = np.exp(log_r_NK)\n",
    "        # TODO update r_NK to optimal value given current GMM parameters\n",
    "        # denom = logsumexp(self.log_pi_K[np.newaxis, :]+np.sum(stats.norm.logpdf(x_ND[:, np.newaxis, :], self.mu_KD, self.stddev_KD), axis=2), axis=1, keepdims=True)\n",
    "        # for i in range(self.K):\n",
    "        #     num = self.log_pi_K[i] + np.sum(stats.norm.logpdf(x_ND, self.mu_KD[i], self.stddev_KD[i]))\n",
    "        #     r_NK[:,i] = np.exp(num - denom)\n",
    "        stacked_denom = []\n",
    "        denom = logsumexp(self.log_pi_K[np.newaxis, :] + np.sum(stats.norm.logpdf(x_ND[:, np.newaxis, :], self.mu_KD, self.stddev_KD),axis = 2),axis = 1)\n",
    "        for i in range(self.K):\n",
    "            log_pdf_ND = stats.norm.logpdf(x_ND, loc=self.mu_KD[i], scale=self.stddev_KD[i])\n",
    "            num = np.sum(log_pdf_ND, axis = 1)\n",
    "            r_NK[:, i] = self.log_pi_K[i] + num\n",
    "            stacked_denom.append(denom)\n",
    "        stacked_denom_ = np.column_stack(stacked_denom)\n",
    "        r_NK  = r_NK - stacked_denom_\n",
    "        r_NK = np.exp(r_NK)\n",
    "        assert np.allclose(np.sum(r_NK, axis=1), 1.0)\n",
    "        return r_NK\n",
    "\n",
    "    def mstep__update_log_pi_K(self, r_NK):\n",
    "        ''' Perform M-step to update mixture weights pi\n",
    "\n",
    "        Returned value will optimize the EM loss function for log_pi_K given fixed other parameters\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        r_NK : 2D array, shape (N, K)\n",
    "            The n-th row r_NK[n] defines the K-length vector r_n that is non-negative & sums to one.\n",
    "            Can interpret r_NK[n,k] as the probability of assigning cluster k to n-th example\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_pi_K : 1D array, shape (K,)\n",
    "            GMM parameter: Log of mixture weights\n",
    "            Must satisfy logsumexp(log_pi_K) == 0.0 (which means sum(exp(log_pi_K)) == 1.0)\n",
    "        '''\n",
    "        # TODO compute optimal update of log_pi_K (hint, update pi_K, then log)\n",
    "        log_pi_K = np.log(np.sum(r_NK, axis = 0)/r_NK.shape[0]) # FIXME\n",
    "        return log_pi_K\n",
    "\n",
    "    def mstep__update_mu_KD(self, r_NK, x_ND):\n",
    "        ''' Perform M-step to update component means mu\n",
    "\n",
    "        Returned value will optimize the EM loss function for mu_KD given fixed other parameters\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        r_NK : 2D array, shape (N, K)\n",
    "            The n-th row r_NK[n] defines the K-length vector r_n that is non-negative & sums to one.\n",
    "            Can interpret r_NK[n,k] as the probability of assigning cluster k to n-th example\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu_KD : 2D array, shape (K, D)\n",
    "            GMM parameter: Means of all components\n",
    "            The k-th row is the mean vector for the k-th component\n",
    "        '''\n",
    "        ## TODO compute optimal update of mu_KD\n",
    "        mu_KD = np.zeros((self.K, self.D)) # FIXME\n",
    "        # for i in range(self.K):\n",
    "        #     num = r_NK.T @ x_ND\n",
    "        #     denom = np.sum(r_NK, axis = 0)\n",
    "        #     mu_KD[i,:] = num[i,:] / denom[i]\n",
    "        for i in range(self.K):\n",
    "            mu_KD[i,:] = (r_NK[:,i].T @ x_ND) / np.sum(r_NK[:,i])\n",
    "        return mu_KD\n",
    "\n",
    "    def mstep__update_stddev_KD(self, r_NK, x_ND):\n",
    "        ''' Perform M-step to update component stddev parameters sigma\n",
    "\n",
    "        Returned value will optimize the EM loss function for stddev_KD given fixed other parameters\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        r_NK : 2D array, shape (N, K)\n",
    "            The n-th row r_NK[n] defines the K-length vector r_n that is non-negative & sums to one.\n",
    "            Can interpret r_NK[n,k] as the probability of assigning cluster k to n-th example\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stddev_KD : 2D array, shape (K, D)\n",
    "            GMM parameter: Standard Deviations of all components\n",
    "            The k-th row is the stddev vector for the k-th component\n",
    "        '''\n",
    "        ## TODO compute optimal update of stddev_KD\n",
    "        stddev_KD = np.ones((self.K, self.D))\n",
    "        for i in range(self.K):\n",
    "            num = (1 / (self.variance_penalty_spread * self.variance_penalty_mode)) + r_NK[:,i].T @ np.square(x_ND - self.mu_KD[i,:])\n",
    "            denom = (1 / (self.variance_penalty_spread * self.variance_penalty_mode * self.variance_penalty_mode)) + np.sum(r_NK[:,i])\n",
    "            stddev_KD[i,:] = np.sqrt(num/denom)\n",
    "    \n",
    "        return stddev_KD\n",
    "\n",
    "    def fit(self, x_ND, x_valid_ND=None, verbose=True):\n",
    "        ''' Fit this estimator to provided training data using EM algorithm\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        x_ND : 2D array, shape (N, D)\n",
    "            Dataset used for training.\n",
    "            Each row is an observed feature vector of size D\n",
    "        x_valid_ND : 2D array, shape (Nvalid, D), optional\n",
    "            Optional, dataset used for heldout validation.\n",
    "            Each row is an observed feature vector of size D\n",
    "            If provided, used to measure heldout likelihood at every checkpoint.\n",
    "            These likelihoods will be recorded in self.history['valid_score_per_pixel']\n",
    "        verbose : boolean, optional, defaults to True\n",
    "            If provided, a message will be printed to stdout after every iteration,\n",
    "            indicating the current training loss and (if possible) validation score.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : this GMM object\n",
    "            Internal attributes log_pi_K, mu_KD, stddev_KD updated.\n",
    "            Performance metrics stored after every iteration in history \n",
    "        '''\n",
    "        N = np.maximum(x_ND.shape[0], 1.0)\n",
    "\n",
    "        ## Define initial parameters\n",
    "        if not hasattr(self, 'log_pi_K'):\n",
    "            self.log_pi_K, self.mu_KD, self.stddev_KD = self.generate_initial_parameters(x_ND)\n",
    "\n",
    "        self.history = defaultdict(list)\n",
    "        start_time_sec = time.time()\n",
    "        for iter_id in range(self.max_iter + 1):\n",
    "\n",
    "            ## Loss step\n",
    "            ND = N * x_ND.shape[1]\n",
    "            tr_score = self.score(x_ND)\n",
    "            loss_with_penalty = -1.0 * tr_score + self.calc_penalty_stddev()\n",
    "            self.history['iter'].append(iter_id)\n",
    "            self.history['train_loss'].append(loss_with_penalty)\n",
    "            self.history['train_loss_per_pixel'].append(loss_with_penalty / ND)\n",
    "            self.history['train_score_per_pixel'].append(tr_score / ND)\n",
    "\n",
    "            if x_valid_ND is None:\n",
    "                va_score_message = \"\"\n",
    "            else:\n",
    "                # TODO compute the per-pixel negative log likelihood on validation set\n",
    "                va_score_per_pixel = calc_neg_log_lik(x_valid_ND, self.log_pi_K, self.mu_KD, self.stddev_KD) # FIXME\n",
    "                self.history['valid_score_per_pixel'].append(va_score_per_pixel)\n",
    "                va_score_message = \"| valid score %9.6f\" % (self.history['valid_score_per_pixel'][-1])\n",
    "\n",
    "            ## E step\n",
    "            r_NK = self.estep__calc_r_NK(x_ND)\n",
    "            if self.do_double_check_correctness:\n",
    "                # Verify the loss after E step is equal to the incomplete loss\n",
    "                loss_e = self.calc_EM_loss(r_NK, x_ND)\n",
    "                self.history['train_loss_em'].append(loss_e)\n",
    "                assert np.allclose(loss_with_penalty, loss_e)\n",
    "                ## TODO this should pass: assert np.allclose(loss_with_penalty, loss_e)\n",
    "\n",
    "            ## M step\n",
    "            if r_NK.shape[0] > 1:\n",
    "                self.log_pi_K = self.mstep__update_log_pi_K(r_NK)\n",
    "                self.mu_KD = self.mstep__update_mu_KD(r_NK, x_ND)\n",
    "            self.stddev_KD = self.mstep__update_stddev_KD(r_NK, x_ND)\n",
    "\n",
    "            if self.do_double_check_correctness:\n",
    "                # Verify the loss goes down after the M step\n",
    "                loss_m = self.calc_EM_loss(r_NK, x_ND)\n",
    "                self.history['train_loss_em'].append(loss_m)\n",
    "                assert loss_m <= loss_e + 1e-9\n",
    "                ## TODO this should pass: assert loss_m <= loss_e + 1e-9\n",
    "\n",
    "            if verbose:\n",
    "                print(\"iter %4d / %4d after %9.1f sec | train loss % 9.6f %s\" % (\n",
    "                    iter_id, self.max_iter, time.time() - start_time_sec,\n",
    "                    self.history['train_loss_per_pixel'][-1],\n",
    "                    va_score_message,\n",
    "                    ))\n",
    "            # The iteration stops when\n",
    "            # (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol.\n",
    "            if iter_id >= 2:\n",
    "                fnew = self.history['train_loss'][-1]\n",
    "                fold = self.history['train_loss'][-2]\n",
    "                numer = (fold - fnew)\n",
    "                denom = np.max(np.abs([fnew, fold, 1]))\n",
    "                if numer / denom <= self.ftol:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em = GMM_PenalizedMLEstimator_EM(K=3, D=2, seed=42, variance_penalty_mode=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False, precision=3, linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False, precision=3, linewidth=80)\n",
    "D = 2\n",
    "empty_ND = np.zeros((0,D))\n",
    "log_pi_K, mu_KD, stddev_KD = gmm_em.generate_initial_parameters(empty_ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.099 -1.099 -1.099]\n",
      "[[2.759 2.449]\n",
      " [2.261 1.635]\n",
      " [1.635 1.496]]\n",
      "[[ 0.921 -0.337]\n",
      " [-0.394  0.514]\n",
      " [-0.14  -0.22 ]]\n"
     ]
    }
   ],
   "source": [
    "print(log_pi_K)\n",
    "print(stddev_KD)\n",
    "print(mu_KD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_neg_log_lik(empty_ND, log_pi_K, mu_KD, stddev_KD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25; K = 3\n",
    "prng = np.random.RandomState(8675309)\n",
    "x1_ND = 0.1 * prng.randn(N, D) + np.asarray([[0, 0]])\n",
    "x2_ND = 0.1 * prng.randn(N, D) + np.asarray([[-1, 0]])\n",
    "x3_ND = np.asarray([[0.2, 0.05]]) * prng.randn(N, D) + np.asarray([[0, +1]])\n",
    "x_ND = np.vstack([x1_ND, x2_ND, x3_ND])\n",
    "gmm_em = GMM_PenalizedMLEstimator_EM(K=3, D=2, seed=42, variance_penalty_mode=2.0, max_iter=1)\n",
    "gmm_em.stddev_KD = 0.1 * np.ones((K,D))\n",
    "gmm_em.stddev_KD[-1] = [0.2, 0.05]\n",
    "gmm_em.mu_KD = np.asarray([[0, 0], [-1., 0], [0, 1.]])\n",
    "gmm_em.log_pi_K = np.log(1./3 * np.ones(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 5.336e-25, 3.829e-75],\n",
       "       [1.000e+00, 2.151e-17, 3.063e-97],\n",
       "       [1.000e+00, 4.367e-19, 1.984e-90]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_em.estep__calc_r_NK(x_ND[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.752e-25, 1.362e-38, 1.000e+00],\n",
       "       [2.278e-17, 7.579e-46, 1.000e+00],\n",
       "       [4.189e-22, 4.117e-34, 1.000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_em.estep__calc_r_NK(x_ND[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.fit(x_ND, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.333, 0.333, 0.333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(gmm_em.log_pi_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.007,  0.01 ],\n",
       "       [-1.008,  0.009],\n",
       "       [-0.005,  1.005]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_em.mu_KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.076, 0.091],\n",
       "       [0.098, 0.103],\n",
       "       [0.24 , 0.042]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_em.stddev_KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_NK = gmm_em.estep__calc_r_NK(empty_ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i in range(gmm_em.K):\n",
    "    print(r_NK[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_ND.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 3), dtype=float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_NK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em = GMM_PenalizedMLEstimator_EM(K=3, D=2, seed=42, variance_penalty_mode=2, max_iter=1000,do_double_check_correctness=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.fit(x_ND, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.333, 0.333, 0.333])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(gmm_em.log_pi_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.008,  0.009],\n",
       "       [-0.005,  1.005],\n",
       "       [-0.007,  0.01 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_em.mu_KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.098, 0.103],\n",
       "       [0.24 , 0.042],\n",
       "       [0.076, 0.091]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_em.stddev_KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('spr_2021s_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eaebc6faa0b4b1811423fff0aa38b2cf938ee2a81b437b69e9ff7acc8a460816"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
