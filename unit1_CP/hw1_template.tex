\documentclass[10pt]{article}

%%% Doc layout
\usepackage{fullpage} 
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}      % microtypography
\usepackage{parskip}
\usepackage{times}

%% Hyperlinks always black, no weird boxes
\usepackage[hyphens]{url}
\usepackage[colorlinks=true,allcolors=black,pdfborder={0 0 0}]{hyperref}

%%% Math typesetting
\usepackage{amsmath,amssymb}

%%% Write out problem statements in blue, solutions in black
\usepackage{xcolor}
\newcommand{\officialdirections}[1]{{\color{purple} #1}}

%%% Avoid automatic section numbers (we'll provide our own)
\setcounter{secnumdepth}{0}

%% --------------
%% Header
%% --------------
\usepackage{fancyhdr}
\fancyhf{}
\fancyhead[C]{\ifnum\value{page}=1 CS 136 - 2023s - HW1 Submission \else \fi}
\fancyfoot[C]{\thepage} % page number
\renewcommand\headrulewidth{0pt}
\pagestyle{fancy}


%% --------------
%% Begin Document
%% --------------
\begin{document}

~~\\ %% add vertical space

{\Large{\bf Student Name: Guangyu Lin}}

\Large{\bf Collaboration Statement:}

Total hours spent: 4 hours

I discussed ideas with these individuals:
\begin{itemize}
\item TODO
\item TODO
\item $\ldots$	
\end{itemize}

I consulted the following resources:
\begin{itemize}
\item office hour with Professor
\item TODO
\item $\ldots$	
\end{itemize}
~~\\
By submitting this assignment, I affirm this is my own original work that abides by the course collaboration policy.
~~\\
~~\\
Links: 
\href{https://www.cs.tufts.edu/cs/136/2023s/hw1.html}{[HW1 instructions]} 
\href{https://www.cs.tufts.edu/cs/136/2023s/index.html#collaboration}{[collab. policy]} 

\tableofcontents

\newpage

\officialdirections{
\subsection*{1a: Problem Statement}

Let $\rho \in (0.0, 1.0)$ be a Beta-distributed random variable: $p \sim \text{Beta}(a, b)$. 

Show that $\mathbb{E}[ \rho ] = \frac{a}{a + b}$.

**Hint:** You can use these identities, which hold for all $a > 0$ and $b > 0$:

\begin{align}
\Gamma(a) &= \int_{t=0}^{\infty} e^{-t} t^{a-1} dt
\\
\Gamma(a+1) &= a \Gamma(a)
\\
\int_{0}^1 \rho^{a-1} (1-\rho)^{b-1} d\rho &= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
\end{align}
}

\subsection{1a: Solution}

$\mathbb{E}[ \rho ]$
\\
\\
= $\mathbb{E}[ Beta(a,b) ]$ substitute the beta pdf by definition
\\
\\
= $\Sigma \mu c(a,b)\mu^{a-1}(1-\mu)^{b-1}$ move the $c(a,b)$ outside of the function
\\
\\
=$c(a,b)\Sigma \mu^{a}\mu^{b-1}$ by gamma function's identity 3
\\
\\
= $\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\cdot\frac{\Gamma(a+1)\cdot\Gamma(b)}{\Gamma(a+b+1)}$ by using the second identity
\\
\\
= $\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\cdot\frac{a\Gamma(a)\Gamma(b)}{(a+b)\Gamma(a+b)}$ by cancel some terms we can get
\\
\\
=$\frac{a}{a+b}$


\officialdirections{
\subsection*{1b: Problem Statement}

Let $\mu$ be a Dirichlet-distributed random variable: $\mu \sim \text{Dir}(a_1, \ldots a_V)$. 

Show that $\mathbb{E}[ \mu_w ] = \frac{a_w}{\sum_{v=1}^V a_v}$, for any integer $w$ that indexes a vocabulary word.

** Hint:** You can use the identity:
\begin{align}
\int \mu_1^{a_1-1} \mu_2^{a_2 - 1} \ldots \mu_V^{a_V-1} d\mu
 &= \frac
 	{\prod_{v=1}^V \Gamma(a_v)}
 	{\Gamma(a_1 + a_2 \ldots + a_V)}
\end{align}
}

\subsection{1b: Solution}
$\mathbb{E}[ \mu_w ]$
\\
\\
= $\Sigma \mu$DirPDF($\mu|a$) substitute Dirichlet distribution by its definition
\\
\\
= $\Sigma c(a)\prod_{v=1}^V\mu^{a_v}$ move the c(a) outside of the function because it is a constant
\\
\\
= $c(a) \Sigma\prod_{v=1}^V\mu^{a_v}$ using Dirichlet distribution's identity
\\
\\
= $\frac{\Gamma(\Sigma a_v)}{\prod_{v}\Gamma(a_v)}\cdot\frac{\prod\Gamma(a_v+1)}{\Gamma(\Sigma a_v+1)}$ using gamma function identity
\\
\\
=$\frac{\Gamma(\Sigma a_v)}{\prod_{v}\Gamma(a_v)}\cdot\frac{a_v\prod\Gamma(a_v)}{\Sigma a_v\Gamma(\Sigma a_v)}$ cancel some terms and we get 
\\
\\
=$\frac{a_v}{\Sigma a_v}$


\officialdirections{
\subsection*{2a: Problem Statement}

Show that the likelihood of all $N$ observed words can be written as:
\begin{align}
p(X_1 = x_1, X_2 = x_2, \ldots, X_N = x_N | \mu) = \prod_{v=1}^V \mu_v^{n_v}
\end{align}
}

\subsection{2a: Solution}
$p(X_1 = x_1, ..., X_n = x_n | \mu)$
\\
\\
= $\prod_{n=1}^NCatPMF(X_n|\mu)$ \text{by the definition of categorical distribution}
\\
\\
= $\prod_{n=1}^N\prod_{v=1}^V\mu^{X_{nv}}$ because $n_v = \Sigma_{n=1}^N[X_n = V]$ so we can get 
\\
\\
=$\prod_{v=1}^V\mu_{v}^{n_{v}}$


\officialdirections{
\subsection*{2b: Problem Statement}

Derive the next-word posterior predictive, after integrating away parameter $\mu$.

That is, show that after seeing the $N$ training words, the probability of the next word $X_*$ being vocabulary word $v$ is:
\begin{align}
p( X_* = v | X_1 = x_1 \ldots X_N = x_n)
	&= \int p( X_* = v, \mu | X_1 = x_1 \ldots X_N = x_n) d\mu
\notag \\
	&= \frac{n_v + \alpha}{N + V\alpha}
\end{align}
}

\subsection{2b: Solution}
$\int p( X_* = v, \mu | X_1 = x_1 \ldots X_N = x_n) d\mu$ \text{using product rule we can decompose it into two parts}
\\
\\
= $\int p(X_*|\mu, X_1,\ldots, X_n)p(\mu|X_1,\ldots, X_n)d\mu$ \text{the first part can be written as} $p(x_*|\mu)$ \text{and the second part can be derived as DirPDF due to the conditional independence}
\\
\\
=$\int Cat(x_*=v|\mu)\cdot DirPDF(\mu|\alpha + n)d\mu$ substitute by each distribution's definition
\\
\\
=$\int \mu_v \cdot c(\alpha + n) \cdot \prod_{v=1}^V\mu_v^{\alpha_v + n_v -1}d\mu$ moved the constant part outside of the integral
\\
\\
=$c(\alpha + n) \int \mu_v \cdot \prod_{v=1}^V \mu^{\alpha_v + n_v -1}d\mu$ we can create $\beta$ as a new vector that contains $[\alpha_1, \alpha_2, ..., \alpha_v + 1, \alpha_V]$ then we have 
\\
\\
=$c(\alpha + n)\int \mu_v \cdot \prod_{v=1}^V \mu ^ {(\beta_v + n_v -1)}d\mu$ then we can use the identity of dirichlet distribution
\\
\\
= $\frac {\Gamma(\Sigma_{v} \alpha_v + n_v)}{\prod_{v} \Gamma(\alpha_v+n_v)} \cdot \frac{\prod_{v} \Gamma(\beta_v+n_v)}{\Gamma(\Sigma_v \beta_v+n_v)}$ then we substitue the $\beta_v$ as $\alpha_v + 1$ and using the identity of gamma function we can get
\\
\\
= $\frac {\Gamma(\Sigma_{v} \alpha_v + n_v)}{\prod_{v} \Gamma(\alpha_v+n_v)} \cdot \frac{(n_v + \alpha_v)\prod_v\Gamma(\alpha_v+n_v)}{\Sigma_v \alpha_v + n_v \Gamma(\Sigma_v \alpha_v + n_v)}$ we can cancel some terms and $\Sigma_v \alpha_v$ can be written as $V\alpha$ and $\Sigma_v n$ can be written as N and we get
\\
\\
= $\frac{n_v + \alpha}{N + V\alpha}$


\officialdirections{
\subsection*{2c: Problem Statement}
Derive the marginal likelihood of observed training data, after integrating away the parameter $\mu$.

That is, show that the marginal probability of the observed $N$ training words has the following closed-form expression:
\begin{align}
p( X_1 = x_1 \ldots X_N = x_N) 
	&= \int p( X_1 = x_1, \ldots X_N = x_N, \mu ) d\mu
	\\
	&= \frac
	{ \Gamma(V \alpha)      \prod_{v=1}^V \Gamma( n_v + \alpha ) }
	{ \Gamma(N + V \alpha ) \prod_{v=1}^V \Gamma(\alpha)         }
\end{align}
}


\subsection{2c: Solution}
$\int p(X_1 = x_1, ..., X_n = x_n, \mu) d\mu$ using Bayes Rule we can get 
\\
\\
= $\int p(X_1 = x_1, ..., X_n = x_n | \mu) \cdot p(\mu) d\mu$ substitute the first term by the result of 2a and the second part is just the dirichelet distribution
\\
\\
=$\int \prod_{v=1}^V \mu_v^{n_v} \cdot DirPDF(\mu | \alpha) d\mu$ so we can substitue with the definition of dirichlet distribution and we can get 
\\
\\
= $\int \prod_{v=1}^V\mu_v^{n_v} \cdot c(\alpha) \cdot \prod_{v=1}^V\mu_v^{\alpha_v-1}$ we can move the constant part outside and combine these two terms 
\\
\\
=$c(\alpha) \int \prod_{v=1}^V \mu_v^{\alpha_v + n_v -1}d\mu$ then we can use the identity of dirichlet distribution and get
\\
\\
= $\frac{\Gamma(\Sigma_v \alpha_v)}{\prod_{v=1}^V\Gamma(\alpha_v)} \cdot \frac{\prod_{v=1}^V\Gamma(\alpha_v + n_v)}{\Gamma(\Sigma_v \alpha_v + n_v)}$ then $\Sigma_v \alpha_v$ can be written as $V\alpha$ and $\Sigma_v \alpha_v + n_v$ can be written as $V\alpha + N$ then we get 
\\
\\
= $\frac{\Gamma(V\alpha) \cdot \prod_{v=1}^V\Gamma(\alpha_v + n_v)}{\Gamma(N+V\alpha) \cdot \prod_{v=1}^V\Gamma(\alpha_v)}$


\end{document}
